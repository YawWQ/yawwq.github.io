---
layout: post
title:  "强化学习"
date:   2019-12-11"
excerpt: "The reinforcement learner has to try out different strategies and see which work best. "
tag:
- 机器学习
- 强化学习
comments: false
---

## 1.概述

强化学习是状态（state）或情形（situation）与动作（action）之间的映射，目的是最大化一些数值形式的奖赏，通过不断的学习找到规律。

## 2.状态和动作空间

学习器需要经历的所有可能的状态集合叫作状态空间（state space），有一个相对应的动作空间（action space）包含所有可能的动作。

## 3.奖赏函数

奖赏函数类似遗传算法中的适应度函数。对于情节性的（episodic）问题，学习过程将被分为许多情节，并且有一个明确的终点，比如机器人到达迷宫中心。而对于非情节性的例子（连续任务（continual task）），并且在任务停止前没有间断，比如婴儿学走路，当它不会跌倒时才算是真的学习成功，而不是10分钟内不会跌倒。

激励学习的是总共的奖赏，即从开始一直到任务结束的期望奖赏（当学习器到达最终状态（terminal state）或接受状态（accepting state））。然而，连续任务没有终结状态，所以要预测奖赏一直到无限的情况下其实是不可能的。

## 4.折扣

折扣（discounting）意味着我们要考虑到对于发生在未来的事有多确定：根据学习中出错的概率对未来奖赏的预测打折扣，我们可以加入额外的参数0 ≤ γ ≤ 1，通过乘以$γ^t$来对未来奖赏的预测打折，t是奖赏中未来的时间步数，γ小于1，所以γ的n次方就更小，所以可以忽略未来很远的预测。

<center>$$R_t = r_{t+1} + \gamma r_{t+2} + {\gamma}^2r_{t+3} + ... + {\gamma}^{k-1}r_{t+k} + ... = {\sum_{k=0}{\infty}γ^{k}r_{t+k+1}}$$    (4.1)</center>

$\gamma$越接近0，也就对未来价值看得越近，当它等于1，就没有折扣。

## 5.动作选择

强化学习中，算法会查看中当前状态下可以进行的每个动作并计算每个动作的值，也就是在当前状态选择这个动作的平均奖赏，可以通过计算以前记录的每次奖赏平均值来实现，这就是$Q_{s,t}(a)$，其中s是状态，a是动作，t是以前这个动作在这个状态下被选择的次数。

三种选择a的方法：

1. 贪婪法：选择$Q_{s,t}(a)$最高的动作，利用当前知识

2. $\epsilon$-greedy：与贪婪算法相似，但有小概率会随机选择其他动作

3. soft-max：

<center>$$P(Q_{s,t}(a))=\frac{exp(\frac{Q_{s,t}(a)}{\tau})}{\sum_{b}exp(\frac{Q_{s,t}(a)}{\tau})}$$    (1.2)</center>

利用soft-max函数来做出选择。引入新参数$\tau$表示温度，与模拟退火有关系，当它很大时所有动作都有相似的概率；当它很小时，选项的概率就更重要。在soft-max中，大多时间会像贪婪一样，但有些情况会按照与它们估计的奖赏比率来选择，并在选择之后更新。

## 6.策略

动作选择旨在利用探索和开发的方法来最大化未来的期望奖赏，而另一方面，其实可以用明确的决策来时每个阶段都做最好的选择，考虑在每个状态选择哪个动作来使得结果更加优化，这就是策略（policy）。


References:

Marsland S. Machine learning: an algorithmic perspective[M]. Chapman and Hall/CRC, 2014.
